<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML3.2 EN">
<HTML>
<HEAD>
<META NAME="GENERATOR" CONTENT="DOCTEXT">
<TITLE>MatCreateMPIBAIJ</TITLE>
</HEAD>
<BODY BGCOLOR="FFFFFF">
<A NAME="MatCreateMPIBAIJ"><H1>MatCreateMPIBAIJ</H1></A>
Creates a sparse parallel matrix in block AIJ format (block compressed row).  For good matrix assembly performance the user should preallocate the matrix storage by setting the parameters  d_nz (or d_nnz) and o_nz (or o_nnz).  By setting these parameters accurately, performance can be increased by more than a factor of 50. 
<H3><FONT COLOR="#CC3333">Synopsis</FONT></H3>
<PRE>
#include "petscmat.h"  
PetscErrorCode PETSCMAT_DLLEXPORT MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt d_nz,const PetscInt d_nnz[],PetscInt o_nz,const PetscInt o_nnz[],Mat *A)
</PRE>
Collective on <A HREF="../Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>
<P>
<H3><FONT COLOR="#CC3333">Input Parameters</FONT></H3>
<TABLE border="0" cellpadding="0" cellspacing="0">
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>comm </B></TD><TD>- MPI communicator
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>bs   </B></TD><TD>- size of blockk
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>m </B></TD><TD>- number of local rows (or <A HREF="../Sys/PETSC_DECIDE.html#PETSC_DECIDE">PETSC_DECIDE</A> to have calculated if M is given)
This value should be the same as the local size used in creating the 
y vector for the matrix-vector product y = Ax.
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>n </B></TD><TD>- number of local columns (or <A HREF="../Sys/PETSC_DECIDE.html#PETSC_DECIDE">PETSC_DECIDE</A> to have calculated if N is given)
This value should be the same as the local size used in creating the 
x vector for the matrix-vector product y = Ax.
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>M </B></TD><TD>- number of global rows (or <A HREF="../Sys/PETSC_DETERMINE.html#PETSC_DETERMINE">PETSC_DETERMINE</A> to have calculated if m is given)
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>N </B></TD><TD>- number of global columns (or <A HREF="../Sys/PETSC_DETERMINE.html#PETSC_DETERMINE">PETSC_DETERMINE</A> to have calculated if n is given)
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>d_nz  </B></TD><TD>- number of nonzero blocks per block row in diagonal portion of local 
submatrix  (same for all local rows)
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>d_nnz </B></TD><TD>- array containing the number of nonzero blocks in the various block rows 
of the in diagonal portion of the local (possibly different for each block
row) or <A HREF="../Sys/PETSC_NULL.html#PETSC_NULL">PETSC_NULL</A>.  You must leave room for the diagonal entry even if it is zero.
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>o_nz  </B></TD><TD>- number of nonzero blocks per block row in the off-diagonal portion of local
submatrix (same for all local rows).
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>o_nnz </B></TD><TD>- array containing the number of nonzero blocks in the various block rows of the
off-diagonal portion of the local submatrix (possibly different for
each block row) or <A HREF="../Sys/PETSC_NULL.html#PETSC_NULL">PETSC_NULL</A>.
</TD></TR></TABLE>
<P>
<H3><FONT COLOR="#CC3333">Output Parameter</FONT></H3>
<DT><B>A </B> -the matrix 
<br>
<P>
<H3><FONT COLOR="#CC3333">Options Database Keys</FONT></H3>
<TABLE border="0" cellpadding="0" cellspacing="0">
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-mat_block_size </B></TD><TD>- size of the blocks to use
</TD></TR>
<TR><TD WIDTH=40></TD><TD ALIGN=LEFT VALIGN=TOP><B>-mat_use_hash_table &lt;fact&gt;</B></TD><TD>- 
It is recommended that one use the <A HREF="../Mat/MatCreate.html#MatCreate">MatCreate</A>(), <A HREF="../Mat/MatSetType.html#MatSetType">MatSetType</A>() and/or <A HREF="../Mat/MatSetFromOptions.html#MatSetFromOptions">MatSetFromOptions</A>(),
MatXXXXSetPreallocation() paradgm instead of this routine directly. 
[MatXXXXSetPreallocation() is, for example, <A HREF="../Mat/MatSeqAIJSetPreallocation.html#MatSeqAIJSetPreallocation">MatSeqAIJSetPreallocation</A>]
</TD></TR></TABLE>
<P>
<H3><FONT COLOR="#CC3333">C++ variants</FONT></H3><TABLE border="0" cellpadding="0" cellspacing="0">
<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,0,PETSC_NULL,0,PETSC_NULL,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,const PetscInt nnz[],const PetscInt onz[],Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,0,nnz,0,onz,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,PetscInt nnz,Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,nz,PETSC_NULL,nnz,PETSC_NULL,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,0,PETSC_NULL,0,PETSC_NULL,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,const PetscInt nnz[],const PetscInt onz[])<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,0,nnz,0,onz,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,PetscInt nnz)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,nz,PETSC_NULL,nnz,PETSC_NULL,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,const PetscInt nnz[],PetscInt onz,const PetscInt onnz[])<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(PETSC_COMM_WORLD,bs,m,n,M,N,nz,nnz,onz,onnz,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,0,PETSC_NULL,0,PETSC_NULL,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,const PetscInt nnz[],const PetscInt onz[],Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,0,nnz,0,onz,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD>&nbsp MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,PetscInt nnz,Mat *A)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,nz,PETSC_NULL,nnz,PETSC_NULL,A)</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,0,PETSC_NULL,0,PETSC_NULL,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,const PetscInt nnz[],const PetscInt onz[])<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,0,nnz,0,onz,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,PetscInt nnz)<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,nz,PETSC_NULL,nnz,PETSC_NULL,&A); return A;</TR></TD>

<TR><TD WIDTH=40></TD><TD> Mat MatCreateMPIBAIJ(MPI_Comm comm,PetscInt bs,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt nz,const PetscInt nnz[],PetscInt onz,const PetscInt onnz[])<TD WIDTH=20></TD><TD>-></TD><TD WIDTH=20></TD><TD>MatCreateMPIBAIJ(comm,bs,m,n,M,N,nz,nnz,onz,onnz,&A); return A;</TR></TD>

</TABLE>
<H3><FONT COLOR="#CC3333">Notes</FONT></H3>
If the *_nnz parameter is given then the *_nz parameter is ignored
<P>
A nonzero block is any block that as 1 or more nonzeros in it
<P>
The user MUST specify either the local or global matrix dimensions
(possibly both).
<P>
If <A HREF="../Sys/PETSC_DECIDE.html#PETSC_DECIDE">PETSC_DECIDE</A> or  <A HREF="../Sys/PETSC_DETERMINE.html#PETSC_DETERMINE">PETSC_DETERMINE</A> is used for a particular argument on one processor
than it must be used on all processors that share the object for that argument.
<P>
<H3><FONT COLOR="#CC3333">Storage Information</FONT></H3>
For a square global matrix we define each processor's diagonal portion
to be its local rows and the corresponding columns (a square submatrix);
each processor's off-diagonal portion encompasses the remainder of the
local matrix (a rectangular submatrix).
<P>
The user can specify preallocated storage for the diagonal part of
the local submatrix with either d_nz or d_nnz (not both).  Set
d_nz=<A HREF="../Sys/PETSC_DEFAULT.html#PETSC_DEFAULT">PETSC_DEFAULT</A> and d_nnz=<A HREF="../Sys/PETSC_NULL.html#PETSC_NULL">PETSC_NULL</A> for PETSc to control dynamic
memory allocation.  Likewise, specify preallocated storage for the
off-diagonal part of the local submatrix with o_nz or o_nnz (not both).
<P>
Consider a processor that owns rows 3, 4 and 5 of a parallel matrix. In
the figure below we depict these three local rows and all columns (0-11).
<P>
<PRE>
           0 1 2 3 4 5 6 7 8 9 10 11
          -------------------
   row 3  |  o o o d d d o o o o o o
   row 4  |  o o o d d d o o o o o o
   row 5  |  o o o d d d o o o o o o
          -------------------
</PRE>

<P>
Thus, any entries in the d locations are stored in the d (diagonal)
submatrix, and any entries in the o locations are stored in the
o (off-diagonal) submatrix.  Note that the d and the o submatrices are
stored simply in the <A HREF="../Mat/MATSEQBAIJ.html#MATSEQBAIJ">MATSEQBAIJ</A> format for compressed row storage.
<P>
Now d_nz should indicate the number of block nonzeros per row in the d matrix,
and o_nz should indicate the number of block nonzeros per row in the o matrix.
In general, for PDE problems in which most nonzeros are near the diagonal,
one expects d_nz &gt;&gt; o_nz.   For large problems you MUST preallocate memory
or you will get TERRIBLE performance; see the users' manual chapter on
matrices.
<P>

<P>
<H3><FONT COLOR="#CC3333">Keywords</FONT></H3>
 matrix, block, aij, compressed row, sparse, parallel
<BR>
<P>
<H3><FONT COLOR="#CC3333">See Also</FONT></H3>
 <A HREF="../Mat/MatCreate.html#MatCreate">MatCreate</A>(), <A HREF="../Mat/MatCreateSeqBAIJ.html#MatCreateSeqBAIJ">MatCreateSeqBAIJ</A>(), <A HREF="../Mat/MatSetValues.html#MatSetValues">MatSetValues</A>(), <A HREF="../Mat/MatCreateMPIBAIJ.html#MatCreateMPIBAIJ">MatCreateMPIBAIJ</A>(), <A HREF="../Mat/MatMPIBAIJSetPreallocation.html#MatMPIBAIJSetPreallocation">MatMPIBAIJSetPreallocation</A>(), <A HREF="../Mat/MatMPIBAIJSetPreallocationCSR.html#MatMPIBAIJSetPreallocationCSR">MatMPIBAIJSetPreallocationCSR</A>()
<BR><P><B><P><B><FONT COLOR="#CC3333">Level:</FONT></B>intermediate
<BR><FONT COLOR="#CC3333">Location:</FONT></B><A HREF="../../../src/mat/impls/baij/mpi/mpibaij.c.html#MatCreateMPIBAIJ">src/mat/impls/baij/mpi/mpibaij.c</A>
<BR><A HREF="./index.html">Index of all Mat routines</A>
<BR><A HREF="../../index.html">Table of Contents for all manual pages</A>
<BR><A HREF="../singleindex.html">Index of all manual pages</A>
</BODY></HTML>
