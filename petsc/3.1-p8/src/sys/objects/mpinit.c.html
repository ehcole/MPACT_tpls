<center><a href="mpinit.c">Actual source code: mpinit.c</a></center><br>

<html>
<head>
<title></title>
<meta name="generator" content="c2html 0.9.5">
<meta name="date" content="2011-03-17T18:43:18+00:00">
</head>

<body bgcolor="#FFFFFF">
<pre width="80">

<a name="line3"> 3: </a> #include <A href="../../../include/petscsys.h.html">petscsys.h</A>

<a name="line5">  5: </a>static <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> saved_PETSC_COMM_WORLD = 0;
<a name="line6">  6: </a><A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> PETSC_COMM_LOCAL_WORLD        = 0;           <font color="#B22222">/* comm for a single node (local set of processes) */</font>
<a name="line7">  7: </a><A href="../../../docs/manualpages/Sys/PetscTruth.html#PetscTruth">PetscTruth</A> PetscOpenMPWorker           = <A href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</A>;  <font color="#B22222">/* this is a regular process, nonworker process */</font>
<a name="line8">  8: </a>void* PetscOpenMPCtx                   = 0;


<a name="line12"> 12: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_COMM_SPAWN)</font>
<a name="line15"> 15: </a><font color="#B22222">/*@C</font>
<a name="line16"> 16: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPSpawn.html#PetscOpenMPSpawn">PetscOpenMPSpawn</A> - Initialize additional processes to be used as "worker" processes. This is not generally </font>
<a name="line17"> 17: </a><font color="#B22222">     called by users. One should use -openmp_spawn_size &lt;n&gt; to indicate that you wish to have n-1 new MPI </font>
<a name="line18"> 18: </a><font color="#B22222">     processes spawned for each current process.</font>

<a name="line20"> 20: </a><font color="#B22222">   Not Collective (could make collective on MPI_COMM_WORLD, generate one huge comm and then split it up)</font>

<a name="line22"> 22: </a><font color="#B22222">   Input Parameter:</font>
<a name="line23"> 23: </a><font color="#B22222">.  nodesize - size of each compute node that will share processors</font>

<a name="line25"> 25: </a><font color="#B22222">   Options Database:</font>
<a name="line26"> 26: </a><font color="#B22222">.   -openmp_spawn_size nodesize</font>

<a name="line28"> 28: </a><font color="#B22222">   Notes: This is only supported on systems with an MPI 2 implementation that includes the MPI_Comm_Spawn() routine.</font>

<a name="line30"> 30: </a><font color="#B22222">$    Comparison of two approaches for OpenMP usage (MPI started with N processes)</font>
<a name="line31"> 31: </a><font color="#B22222">$</font>
<a name="line32"> 32: </a><font color="#B22222">$    -openmp_spawn_size &lt;n&gt; requires MPI 2, results in n*N total processes with N directly used by application code</font>
<a name="line33"> 33: </a><font color="#B22222">$                                           and n-1 worker processes (used by PETSc) for each application node.</font>
<a name="line34"> 34: </a><font color="#B22222">$                           You MUST launch MPI so that only ONE MPI process is created for each hardware node.</font>
<a name="line35"> 35: </a><font color="#B22222">$</font>
<a name="line36"> 36: </a><font color="#B22222">$    -openmp_merge_size &lt;n&gt; results in N total processes, N/n used by the application code and the rest worker processes</font>
<a name="line37"> 37: </a><font color="#B22222">$                            (used by PETSc)</font>
<a name="line38"> 38: </a><font color="#B22222">$                           You MUST launch MPI so that n MPI processes are created for each hardware node.</font>
<a name="line39"> 39: </a><font color="#B22222">$</font>
<a name="line40"> 40: </a><font color="#B22222">$    petscmpiexec -n 2 ./ex1 -openmp_spawn_size 3 gives 2 application nodes (and 4 PETSc worker nodes)</font>
<a name="line41"> 41: </a><font color="#B22222">$    petscmpiexec -n 6 ./ex1 -openmp_merge_size 3 gives the SAME 2 application nodes and 4 PETSc worker nodes</font>
<a name="line42"> 42: </a><font color="#B22222">$       This is what would use if each of the computers hardware nodes had 3 CPUs.</font>
<a name="line43"> 43: </a><font color="#B22222">$</font>
<a name="line44"> 44: </a><font color="#B22222">$      These are intended to be used in conjunction with USER OpenMP code. The user will have 1 process per</font>
<a name="line45"> 45: </a><font color="#B22222">$   computer (hardware) node (where the computer node has p cpus), the user's code will use threads to fully</font>
<a name="line46"> 46: </a><font color="#B22222">$   utilize all the CPUs on the node. The PETSc code will have p processes to fully use the compute node for </font>
<a name="line47"> 47: </a><font color="#B22222">$   PETSc calculations. The user THREADS and PETSc PROCESSES will NEVER run at the same time so the p CPUs </font>
<a name="line48"> 48: </a><font color="#B22222">$   are always working on p task, never more than p.</font>
<a name="line49"> 49: </a><font color="#B22222">$</font>
<a name="line50"> 50: </a><font color="#B22222">$    See <A href="../../../docs/manualpages/PC/PCOPENMP.html#PCOPENMP">PCOPENMP</A> for a PETSc preconditioner that can use this functionality</font>
<a name="line51"> 51: </a><font color="#B22222">$</font>

<a name="line53"> 53: </a><font color="#B22222">   For both <A href="../../../docs/manualpages/Sys/PetscOpenMPSpawn.html#PetscOpenMPSpawn">PetscOpenMPSpawn</A>() and <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>() <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> consists of one process per "node", PETSC_COMM_LOCAL_WORLD</font>
<a name="line54"> 54: </a><font color="#B22222">   consists of all the processes in a "node."</font>

<a name="line56"> 56: </a><font color="#B22222">   In both cases the user's code is running ONLY on <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> (that was newly generated by running this command).</font>

<a name="line58"> 58: </a><font color="#B22222">   Level: developer</font>

<a name="line60"> 60: </a><font color="#B22222">   Concepts: OpenMP</font>
<a name="line61"> 61: </a><font color="#B22222">   </font>
<a name="line62"> 62: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), PetscInitializeFortran(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPFinalize.html#PetscOpenMPFinalize">PetscOpenMPFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscInitialize.html#PetscInitialize">PetscInitialize</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A>()</font>

<a name="line64"> 64: </a><font color="#B22222">@*/</font>
<a name="line65"> 65: </a><strong><font color="#4169E1"><a name="PetscOpenMPSpawn"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPSpawn.html#PetscOpenMPSpawn">PetscOpenMPSpawn</A>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A> nodesize)</font></strong>
<a name="line66"> 66: </a>{
<a name="line68"> 68: </a>  <A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>    size;
<a name="line69"> 69: </a>  <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>       parent,children;
<a name="line70"> 70: </a>
<a name="line72"> 72: </a>  MPI_Comm_get_parent(&amp;parent);
<a name="line73"> 73: </a>  <font color="#4169E1">if</font> (parent == MPI_COMM_NULL) {  <font color="#B22222">/* the original processes started by user */</font>
<a name="line74"> 74: </a>    char programname[PETSC_MAX_PATH_LEN];
<a name="line75"> 75: </a>    char **argv;

<a name="line77"> 77: </a>    <A href="../../../docs/manualpages/Sys/PetscGetProgramName.html#PetscGetProgramName">PetscGetProgramName</A>(programname,PETSC_MAX_PATH_LEN);
<a name="line78"> 78: </a>    <A href="../../../docs/manualpages/Sys/PetscGetArguments.html#PetscGetArguments">PetscGetArguments</A>(&amp;argv);
<a name="line79"> 79: </a>    MPI_Comm_spawn(programname,argv,nodesize-1,MPI_INFO_NULL,0,<A href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</A>,&amp;children,MPI_ERRCODES_IGNORE);
<a name="line80"> 80: </a>    <A href="../../../docs/manualpages/Sys/PetscFreeArguments.html#PetscFreeArguments">PetscFreeArguments</A>(argv);
<a name="line81"> 81: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Intercomm_merge.html#MPI_Intercomm_merge">MPI_Intercomm_merge</A>(children,0,&amp;PETSC_COMM_LOCAL_WORLD);

<a name="line83"> 83: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>(<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>,&amp;size);
<a name="line84"> 84: </a>    PetscInfo2(0,<font color="#666666">"PETSc OpenMP successfully spawned: number of nodes = %d node size = %d\n"</font>,size,nodesize);
<a name="line85"> 85: </a>    saved_PETSC_COMM_WORLD = <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>;
<a name="line86"> 86: </a>  } <font color="#4169E1">else</font> { <font color="#B22222">/* worker nodes that get spawned */</font>
<a name="line87"> 87: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Intercomm_merge.html#MPI_Intercomm_merge">MPI_Intercomm_merge</A>(parent,1,&amp;PETSC_COMM_LOCAL_WORLD);
<a name="line88"> 88: </a>    <A href="../../../docs/manualpages/Sys/PetscOpenMPHandle.html#PetscOpenMPHandle">PetscOpenMPHandle</A>(PETSC_COMM_LOCAL_WORLD);
<a name="line89"> 89: </a>    PetscOpenMPWorker = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>; <font color="#B22222">/* so that PetscOpenMPIFinalize() will not attempt a broadcast from this process */</font>
<a name="line90"> 90: </a>    <A href="../../../docs/manualpages/Sys/PetscEnd.html#PetscEnd">PetscEnd</A>();  <font color="#B22222">/* cannot continue into user code */</font>
<a name="line91"> 91: </a>  }
<a name="line92"> 92: </a>  <font color="#4169E1">return</font>(0);
<a name="line93"> 93: </a>}
<a name="line94"> 94: </a><font color="#A020F0">#endif</font>

<a name="line98"> 98: </a><font color="#B22222">/*@C</font>
<a name="line99"> 99: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A> - Initializes the PETSc and MPI to work with OpenMP. This is not usually called</font>
<a name="line100">100: </a><font color="#B22222">      by the user. One should use -openmp_merge_size &lt;n&gt; to indicate the node size of merged communicator</font>
<a name="line101">101: </a><font color="#B22222">      to be.</font>

<a name="line103">103: </a><font color="#B22222">   Collective on MPI_COMM_WORLD or <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> if it has been set</font>

<a name="line105">105: </a><font color="#B22222">   Input Parameter:</font>
<a name="line106">106: </a><font color="#B22222">+  nodesize - size of each compute node that will share processors</font>
<a name="line107">107: </a><font color="#B22222">.  func - optional function to call on the master nodes</font>
<a name="line108">108: </a><font color="#B22222">-  ctx - context passed to function on master nodes</font>

<a name="line110">110: </a><font color="#B22222">   Options Database:</font>
<a name="line111">111: </a><font color="#B22222">.   -openmp_merge_size &lt;n&gt;</font>

<a name="line113">113: </a><font color="#B22222">   Level: developer</font>

<a name="line115">115: </a><font color="#B22222">$    Comparison of two approaches for OpenMP usage (MPI started with N processes)</font>
<a name="line116">116: </a><font color="#B22222">$</font>
<a name="line117">117: </a><font color="#B22222">$    -openmp_spawn_size &lt;n&gt; requires MPI 2, results in n*N total processes with N directly used by application code</font>
<a name="line118">118: </a><font color="#B22222">$                                           and n-1 worker processes (used by PETSc) for each application node.</font>
<a name="line119">119: </a><font color="#B22222">$                           You MUST launch MPI so that only ONE MPI process is created for each hardware node.</font>
<a name="line120">120: </a><font color="#B22222">$</font>
<a name="line121">121: </a><font color="#B22222">$    -openmp_merge_size &lt;n&gt; results in N total processes, N/n used by the application code and the rest worker processes</font>
<a name="line122">122: </a><font color="#B22222">$                            (used by PETSc)</font>
<a name="line123">123: </a><font color="#B22222">$                           You MUST launch MPI so that n MPI processes are created for each hardware node.</font>
<a name="line124">124: </a><font color="#B22222">$</font>
<a name="line125">125: </a><font color="#B22222">$    petscmpiexec -n 2 ./ex1 -openmp_spawn_size 3 gives 2 application nodes (and 4 PETSc worker nodes)</font>
<a name="line126">126: </a><font color="#B22222">$    petscmpiexec -n 6 ./ex1 -openmp_merge_size 3 gives the SAME 2 application nodes and 4 PETSc worker nodes</font>
<a name="line127">127: </a><font color="#B22222">$       This is what would use if each of the computers hardware nodes had 3 CPUs.</font>
<a name="line128">128: </a><font color="#B22222">$</font>
<a name="line129">129: </a><font color="#B22222">$      These are intended to be used in conjunction with USER OpenMP code. The user will have 1 process per</font>
<a name="line130">130: </a><font color="#B22222">$   computer (hardware) node (where the computer node has p cpus), the user's code will use threads to fully</font>
<a name="line131">131: </a><font color="#B22222">$   utilize all the CPUs on the node. The PETSc code will have p processes to fully use the compute node for </font>
<a name="line132">132: </a><font color="#B22222">$   PETSc calculations. The user THREADS and PETSc PROCESSES will NEVER run at the same time so the p CPUs </font>
<a name="line133">133: </a><font color="#B22222">$   are always working on p task, never more than p.</font>
<a name="line134">134: </a><font color="#B22222">$</font>
<a name="line135">135: </a><font color="#B22222">$    See <A href="../../../docs/manualpages/PC/PCOPENMP.html#PCOPENMP">PCOPENMP</A> for a PETSc preconditioner that can use this functionality</font>
<a name="line136">136: </a><font color="#B22222">$</font>

<a name="line138">138: </a><font color="#B22222">   For both <A href="../../../docs/manualpages/Sys/PetscOpenMPSpawn.html#PetscOpenMPSpawn">PetscOpenMPSpawn</A>() and <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>() <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> consists of one process per "node", PETSC_COMM_LOCAL_WORLD</font>
<a name="line139">139: </a><font color="#B22222">   consists of all the processes in a "node."</font>

<a name="line141">141: </a><font color="#B22222">   In both cases the user's code is running ONLY on <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> (that was newly generated by running this command).</font>

<a name="line143">143: </a><font color="#B22222">   Concepts: OpenMP</font>
<a name="line144">144: </a><font color="#B22222">   </font>
<a name="line145">145: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), PetscInitializeFortran(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPFinalize.html#PetscOpenMPFinalize">PetscOpenMPFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscInitialize.html#PetscInitialize">PetscInitialize</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPSpawn.html#PetscOpenMPSpawn">PetscOpenMPSpawn</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A>()</font>

<a name="line147">147: </a><font color="#B22222">@*/</font>
<a name="line148">148: </a><strong><font color="#4169E1"><a name="PetscOpenMPMerge"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A> nodesize,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*func)(void*),void *ctx)</font></strong>
<a name="line149">149: </a>{
<a name="line151">151: </a>  <A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>    size,rank,*ranks,i;
<a name="line152">152: </a>  MPI_Group      group,newgroup;

<a name="line155">155: </a>  saved_PETSC_COMM_WORLD = <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>;

<a name="line157">157: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</A>(saved_PETSC_COMM_WORLD,&amp;size);
<a name="line158">158: </a>  <font color="#4169E1">if</font> (size % nodesize) <A href="../../../docs/manualpages/Sys/SETERRQ2.html#SETERRQ2">SETERRQ2</A>(PETSC_ERR_ARG_SIZ,<font color="#666666">"Total number of process nodes %d is not divisible by number of processes per node %d"</font>,size,nodesize);
<a name="line159">159: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</A>(saved_PETSC_COMM_WORLD,&amp;rank);


<a name="line162">162: </a>  <font color="#B22222">/* create two communicators </font>
<a name="line163">163: </a><font color="#B22222">      *) one that contains the first process from each node: 0,nodesize,2*nodesize,...</font>
<a name="line164">164: </a><font color="#B22222">      *) one that contains all processes in a node:  (0,1,2...,nodesize-1), (nodesize,nodesize+1,...2*nodesize-), ...</font>
<a name="line165">165: </a><font color="#B22222">  */</font>
<a name="line166">166: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</A>(saved_PETSC_COMM_WORLD,&amp;group);
<a name="line167">167: </a>  <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>((size/nodesize)*<font color="#4169E1">sizeof</font>(<A href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</A>),&amp;ranks);
<a name="line168">168: </a>  <font color="#4169E1">for</font> (i=0; i&lt;(size/nodesize); i++) ranks[i] = i*nodesize;
<a name="line169">169: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_incl.html#MPI_Group_incl">MPI_Group_incl</A>(group,size/nodesize,ranks,&amp;newgroup);
<a name="line170">170: </a>  <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(ranks);
<a name="line171">171: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_create.html#MPI_Comm_create">MPI_Comm_create</A>(saved_PETSC_COMM_WORLD,newgroup,&amp;<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>);
<a name="line172">172: </a>  <font color="#4169E1">if</font> (rank % nodesize) <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> = 0; <font color="#B22222">/* mark invalid processes for easy debugging */</font>
<a name="line173">173: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</A>(&amp;group);
<a name="line174">174: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</A>(&amp;newgroup);

<a name="line176">176: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</A>(saved_PETSC_COMM_WORLD,rank/nodesize,rank % nodesize,&amp;PETSC_COMM_LOCAL_WORLD);

<a name="line178">178: </a>  PetscInfo2(0,<font color="#666666">"PETSc OpenMP successfully started: number of nodes = %d node size = %d\n"</font>,size/nodesize,nodesize);
<a name="line179">179: </a>  PetscInfo1(0,<font color="#666666">"PETSc OpenMP process %sactive\n"</font>,(rank % nodesize) ? <font color="#666666">"in"</font> : <font color="#666666">""</font>);

<a name="line181">181: </a>  PetscOpenMPCtx = ctx;
<a name="line182">182: </a>  <font color="#B22222">/* </font>
<a name="line183">183: </a><font color="#B22222">     All process not involved in user application code wait here</font>
<a name="line184">184: </a><font color="#B22222">  */</font>
<a name="line185">185: </a>  <font color="#4169E1">if</font> (!<A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>) {
<a name="line186">186: </a>    <A href="../../../docs/manualpages/Sys/PetscOpenMPHandle.html#PetscOpenMPHandle">PetscOpenMPHandle</A>(PETSC_COMM_LOCAL_WORLD);
<a name="line187">187: </a>    <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A>  = saved_PETSC_COMM_WORLD;
<a name="line188">188: </a>    PetscOpenMPWorker = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>; <font color="#B22222">/* so that PetscOpenMPIFinalize() will not attempt a broadcast from this process */</font>
<a name="line189">189: </a>    <A href="../../../docs/manualpages/Profiling/PetscInfo.html#PetscInfo">PetscInfo</A>(0,<font color="#666666">"PETSc OpenMP inactive process becoming active"</font>);
<a name="line190">190: </a>  } <font color="#4169E1">else</font> {
<a name="line191">191: </a>    <font color="#4169E1">if</font> (func) {
<a name="line192">192: </a>      (*func)(ctx);
<a name="line193">193: </a>    }
<a name="line194">194: </a>  }
<a name="line195">195: </a>  <font color="#4169E1">return</font>(0);
<a name="line196">196: </a>}

<a name="line200">200: </a><font color="#B22222">/*@C</font>
<a name="line201">201: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPFinalize.html#PetscOpenMPFinalize">PetscOpenMPFinalize</A> - Finalizes the PETSc and MPI to work with OpenMP. Called by <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>() cannot</font>
<a name="line202">202: </a><font color="#B22222">       be called by user.</font>

<a name="line204">204: </a><font color="#B22222">   Collective on the entire system</font>

<a name="line206">206: </a><font color="#B22222">   Level: developer</font>
<a name="line207">207: </a><font color="#B22222">           </font>
<a name="line208">208: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscFinalize.html#PetscFinalize">PetscFinalize</A>(), <A href="../../../docs/manualpages/Sys/PetscGetArgs.html#PetscGetArgs">PetscGetArgs</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), PCOpenMPRun()</font>

<a name="line210">210: </a><font color="#B22222">@*/</font>
<a name="line211">211: </a><strong><font color="#4169E1"><a name="PetscOpenMPFinalize"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPFinalize.html#PetscOpenMPFinalize">PetscOpenMPFinalize</A>(void)</font></strong>
<a name="line212">212: </a>{
<a name="line213">213: </a>  <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> 0;
<a name="line214">214: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 3;

<a name="line217">217: </a>  <font color="#4169E1">if</font> (!PetscOpenMPWorker &amp;&amp; PETSC_COMM_LOCAL_WORLD) {
<a name="line218">218: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,PETSC_COMM_LOCAL_WORLD); <font color="#B22222">/* broadcast to my worker group to end program */</font>
<a name="line219">219: </a>    <A href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</A> = saved_PETSC_COMM_WORLD;
<a name="line220">220: </a>    <A href="../../../docs/manualpages/Profiling/PetscInfo.html#PetscInfo">PetscInfo</A>(0,<font color="#666666">"PETSc OpenMP active process ending <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>()"</font>);
<a name="line221">221: </a>  }
<a name="line222">222: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line223">223: </a>}

<a name="line225">225: </a>static <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A> numberobjects = 0;
<a name="line226">226: </a>static void     *objects[100];

<a name="line230">230: </a><font color="#B22222">/*@C</font>
<a name="line231">231: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPHandle.html#PetscOpenMPHandle">PetscOpenMPHandle</A> - Receives commands from the master node and processes them</font>

<a name="line233">233: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line235">235: </a><font color="#B22222">   Input Parameter:</font>
<a name="line236">236: </a><font color="#B22222">.   comm - Must be PETSC_COMM_LOCAL_WORLD</font>

<a name="line238">238: </a><font color="#B22222">   Level: developer</font>

<a name="line240">240: </a><font color="#B22222">   Notes: this is usually handled automatically, likely you do not need to use this directly</font>

<a name="line242">242: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>
<a name="line243">243: </a><font color="#B22222">           </font>
<a name="line244">244: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), PCOpenMPRun(), PCOpenMPNew()</font>

<a name="line246">246: </a><font color="#B22222">@*/</font>
<a name="line247">247: </a><strong><font color="#4169E1"><a name="PetscOpenMPHandle"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPHandle.html#PetscOpenMPHandle">PetscOpenMPHandle</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm)</font></strong>
<a name="line248">248: </a>{
<a name="line250">250: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command;
<a name="line251">251: </a>  <A href="../../../docs/manualpages/Sys/PetscTruth.html#PetscTruth">PetscTruth</A>     exitwhileloop = <A href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</A>;

<a name="line254">254: </a>  <font color="#4169E1">while</font> (!exitwhileloop) {
<a name="line255">255: </a>    <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line256">256: </a>    <font color="#4169E1">switch</font> (command) {
<a name="line257">257: </a>    <font color="#4169E1">case</font> 0: { <font color="#B22222">/* allocate some memory on this worker process */</font>
<a name="line258">258: </a>      size_t   n;
<a name="line259">259: </a>      void     *ptr;
<a name="line260">260: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;n,1,MPIU_SIZE_T,0,comm);
<a name="line261">261: </a>      <font color="#B22222">/* cannot use <A href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</A>() cause it requires struct argument */</font>
<a name="line262">262: </a>      <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>(n,&amp;ptr);
<a name="line263">263: </a>      <A href="../../../docs/manualpages/Sys/PetscMemzero.html#PetscMemzero">PetscMemzero</A>(ptr,n);
<a name="line264">264: </a>      objects[numberobjects++] = ptr;
<a name="line265">265: </a>      <font color="#4169E1">break</font>;
<a name="line266">266: </a>    }
<a name="line267">267: </a>    <font color="#4169E1">case</font> 1: {  <font color="#B22222">/* free some memory on this worker process */</font>
<a name="line268">268: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A> i;
<a name="line269">269: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line270">270: </a>      <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(objects[i]);
<a name="line271">271: </a>      objects[i] = 0;
<a name="line272">272: </a>      <font color="#4169E1">break</font>;
<a name="line273">273: </a>    }
<a name="line274">274: </a>    <font color="#4169E1">case</font> 2: {  <font color="#B22222">/* run a function on this worker process */</font>
<a name="line275">275: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       i;
<a name="line276">276: </a>      <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*);
<a name="line277">277: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line278">278: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line279">279: </a>      (*f)(comm,objects[i]);
<a name="line280">280: </a>      <font color="#4169E1">break</font>;
<a name="line281">281: </a>    }
<a name="line282">282: </a>    <font color="#4169E1">case</font> 4: {  <font color="#B22222">/* run a function on this worker process with provided context */</font>
<a name="line283">283: </a>      <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       i;
<a name="line284">284: </a>      <A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*,void*);
<a name="line285">285: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line286">286: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line287">287: </a>      (*f)(comm,PetscOpenMPCtx,objects[i]);
<a name="line288">288: </a>      <font color="#4169E1">break</font>;
<a name="line289">289: </a>    }
<a name="line290">290: </a>    <font color="#4169E1">case</font> 3: {
<a name="line291">291: </a>      exitwhileloop = <A href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</A>;
<a name="line292">292: </a>      <font color="#4169E1">break</font>;
<a name="line293">293: </a>    }
<a name="line294">294: </a><strong><font color="#FF0000">    default:</font></strong>
<a name="line295">295: </a>      <A href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</A>(PETSC_ERR_PLIB,<font color="#666666">"Unknown OpenMP command %D"</font>,command);
<a name="line296">296: </a>    }
<a name="line297">297: </a>  }
<a name="line298">298: </a>  <font color="#4169E1">return</font>(0);
<a name="line299">299: </a>}

<a name="line303">303: </a><font color="#B22222">/*@C</font>
<a name="line304">304: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A> - Creates a "c struct" on all nodes of an OpenMP communicator</font>

<a name="line306">306: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line308">308: </a><font color="#B22222">   Input Parameters:</font>
<a name="line309">309: </a><font color="#B22222">+   comm - Must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line310">310: </a><font color="#B22222">-   n  - amount of memory requested</font>

<a name="line312">312: </a><font color="#B22222">   Level: developer</font>

<a name="line314">314: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line316">316: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), PCOpenMPRun(), PCOpenMPFree()</font>

<a name="line318">318: </a><font color="#B22222">@*/</font>
<a name="line319">319: </a><strong><font color="#4169E1"><a name="PetscOpenMPMalloc"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,size_t n,void **ptr)</font></strong>
<a name="line320">320: </a>{
<a name="line322">322: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 0;

<a name="line325">325: </a>  <font color="#4169E1">if</font> (PetscOpenMPWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using OpenMP feature of PETSc"</font>);

<a name="line327">327: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line328">328: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;n,1,MPIU_SIZE_T,0,comm);
<a name="line329">329: </a>  <font color="#B22222">/* cannot use <A href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</A>() cause it requires struct argument */</font>
<a name="line330">330: </a>  <A href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</A>(n,ptr);
<a name="line331">331: </a>  <A href="../../../docs/manualpages/Sys/PetscMemzero.html#PetscMemzero">PetscMemzero</A>(*ptr,n);
<a name="line332">332: </a>  objects[numberobjects++] = *ptr;
<a name="line333">333: </a>  <font color="#4169E1">return</font>(0);
<a name="line334">334: </a>}

<a name="line338">338: </a><font color="#B22222">/*@C</font>
<a name="line339">339: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPFree.html#PetscOpenMPFree">PetscOpenMPFree</A> - Frees a "c struct" on all nodes of an OpenMP communicator</font>

<a name="line341">341: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line343">343: </a><font color="#B22222">   Input Parameters:</font>
<a name="line344">344: </a><font color="#B22222">+   comm - Must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line345">345: </a><font color="#B22222">-   ptr - pointer to data to be freed, must have been obtained with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()</font>

<a name="line347">347: </a><font color="#B22222">   Level: developer</font>
<a name="line348">348: </a><font color="#B22222">           </font>
<a name="line349">349: </a><font color="#B22222">  Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line351">351: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()</font>

<a name="line353">353: </a><font color="#B22222">@*/</font>
<a name="line354">354: </a><strong><font color="#4169E1"><a name="PetscOpenMPFree"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPFree.html#PetscOpenMPFree">PetscOpenMPFree</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,void *ptr)</font></strong>
<a name="line355">355: </a>{
<a name="line357">357: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 1,i;

<a name="line360">360: </a>  <font color="#4169E1">if</font> (PetscOpenMPWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using OpenMP feature of PETSc"</font>);

<a name="line362">362: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line363">363: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line364">364: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line365">365: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line366">366: </a>      <A href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</A>(ptr);
<a name="line367">367: </a>      objects[i] = 0;
<a name="line368">368: </a>      <font color="#4169E1">return</font>(0);
<a name="line369">369: </a>    }
<a name="line370">370: </a>  }
<a name="line371">371: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()"</font>);
<a name="line372">372: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line373">373: </a>}

<a name="line377">377: </a><font color="#B22222">/*@C</font>
<a name="line378">378: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A> - runs a function on all the processes of a node</font>

<a name="line380">380: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line382">382: </a><font color="#B22222">   Input Parameters:</font>
<a name="line383">383: </a><font color="#B22222">+   comm - communicator to run function on, must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line384">384: </a><font color="#B22222">.   f - function to run</font>
<a name="line385">385: </a><font color="#B22222">-   ptr - pointer to data to pass to function; must be obtained with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()</font>

<a name="line387">387: </a><font color="#B22222">   Level: developer</font>
<a name="line388">388: </a><font color="#B22222">           </font>
<a name="line389">389: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line391">391: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPFree.html#PetscOpenMPFree">PetscOpenMPFree</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPRunCtx.html#PetscOpenMPRunCtx">PetscOpenMPRunCtx</A>()</font>

<a name="line393">393: </a><font color="#B22222">@*/</font>
<a name="line394">394: </a><strong><font color="#4169E1"><a name="PetscOpenMPRun"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void *),void *ptr)</font></strong>
<a name="line395">395: </a>{
<a name="line397">397: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 2,i;

<a name="line400">400: </a>  <font color="#4169E1">if</font> (PetscOpenMPWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using OpenMP feature of PETSc"</font>);

<a name="line402">402: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line403">403: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line404">404: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line405">405: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line406">406: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line407">407: </a>      (*f)(comm,ptr);
<a name="line408">408: </a>      <font color="#4169E1">return</font>(0);
<a name="line409">409: </a>    }
<a name="line410">410: </a>  }
<a name="line411">411: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()"</font>);
<a name="line412">412: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line413">413: </a>}

<a name="line417">417: </a><font color="#B22222">/*@C</font>
<a name="line418">418: </a><font color="#B22222">   <A href="../../../docs/manualpages/Sys/PetscOpenMPRunCtx.html#PetscOpenMPRunCtx">PetscOpenMPRunCtx</A> - runs a function on all the processes of a node</font>

<a name="line420">420: </a><font color="#B22222">   Collective on <A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A></font>

<a name="line422">422: </a><font color="#B22222">   Input Parameters:</font>
<a name="line423">423: </a><font color="#B22222">+   comm - communicator to run function on, must be PETSC_COMM_LOCAL_WORLD</font>
<a name="line424">424: </a><font color="#B22222">.   f - function to run</font>
<a name="line425">425: </a><font color="#B22222">-   ptr - pointer to data to pass to function; must be obtained with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()</font>

<a name="line427">427: </a><font color="#B22222">   Notes: This is like <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A>() except it also passes the context passed in <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>()</font>
<a name="line428">428: </a><font color="#B22222">   Level: developer</font>
<a name="line429">429: </a><font color="#B22222">           </font>
<a name="line430">430: </a><font color="#B22222">   Developer Notes: Since comm must be PETSC_COMM_LOCAL_WORLD, why have this argument?</font>

<a name="line432">432: </a><font color="#B22222">.seealso: <A href="../../../docs/manualpages/Sys/PetscOpenMPMerge.html#PetscOpenMPMerge">PetscOpenMPMerge</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPFree.html#PetscOpenMPFree">PetscOpenMPFree</A>(), <A href="../../../docs/manualpages/Sys/PetscOpenMPRun.html#PetscOpenMPRun">PetscOpenMPRun</A>()</font>

<a name="line434">434: </a><font color="#B22222">@*/</font>
<a name="line435">435: </a><strong><font color="#4169E1"><a name="PetscOpenMPRunCtx"></a><A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A>  <A href="../../../docs/manualpages/Sys/PetscOpenMPRunCtx.html#PetscOpenMPRunCtx">PetscOpenMPRunCtx</A>(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A> comm,<A href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</A> (*f)(<A href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</A>,void*,void *),void *ptr)</font></strong>
<a name="line436">436: </a>{
<a name="line438">438: </a>  <A href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</A>       command = 4,i;

<a name="line441">441: </a>  <font color="#4169E1">if</font> (PetscOpenMPWorker) <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONGSTATE,<font color="#666666">"Not using OpenMP feature of PETSc"</font>);

<a name="line443">443: </a>  <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;command,1,MPIU_INT,0,comm);
<a name="line444">444: </a>  <font color="#4169E1">for</font> (i=0; i&lt;numberobjects; i++) {
<a name="line445">445: </a>    <font color="#4169E1">if</font> (objects[i] == ptr) {
<a name="line446">446: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;i,1,MPIU_INT,0,comm);
<a name="line447">447: </a>      <A href="http://www.mcs.anl.gov/mpi/www/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</A>(&amp;f,1,MPIU_SIZE_T,0,comm);
<a name="line448">448: </a>      (*f)(comm,PetscOpenMPCtx,ptr);
<a name="line449">449: </a>      <font color="#4169E1">return</font>(0);
<a name="line450">450: </a>    }
<a name="line451">451: </a>  }
<a name="line452">452: </a>  <A href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</A>(PETSC_ERR_ARG_WRONG,<font color="#666666">"Pointer does not appear to have been created with <A href="../../../docs/manualpages/Sys/PetscOpenMPMalloc.html#PetscOpenMPMalloc">PetscOpenMPMalloc</A>()"</font>);
<a name="line453">453: </a>  <A href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</A>(ierr);
<a name="line454">454: </a>}
</pre>
</body>

</html>
